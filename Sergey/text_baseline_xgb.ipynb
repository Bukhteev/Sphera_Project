{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load normalized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.read_csv('all_docs.csv')\n",
    "data_all = data_all.replace(np.nan, 'noinformation') # have some empty text(the problem had to solve by Sergey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load train and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.read_csv('kirill/train_groups.csv')\n",
    "ts  = pd.read_csv('kirill/test_groups.csv')\n",
    "\n",
    "ts['is_train'] = False\n",
    "tr['is_train'] = True\n",
    "\n",
    "data_x = pd.concat(( tr.drop('target', 1), ts))\n",
    "\n",
    "title = pd.read_csv('kirill/information.csv')\n",
    "satistic = data_x.merge(title, how = 'left', on = 'doc_id')\n",
    "satistic = satistic.replace(np.nan, 'noinformation')\n",
    "\n",
    "numbers = []\n",
    "num = np.arange(10).astype(str)\n",
    "for i in satistic.title:\n",
    "    numbers.append(sum(letter in num for letter in i))\n",
    "    \n",
    "    \n",
    "satistic['numbers'] = numbers\n",
    "\n",
    "\n",
    "vowels = []\n",
    "vowel = ['а', 'о', 'и', 'е', 'ё', 'э', 'ы', 'у', 'ю', 'я']\n",
    "for i in satistic.title:\n",
    "    vowels.append(sum(letter in vowel for letter in i))\n",
    "satistic['vowels'] = vowels\n",
    "\n",
    "\n",
    "consonants = []\n",
    "consonant = ['б', 'в', 'г', 'д', 'ж', 'з', 'й', 'к', 'л', 'м', 'н', 'п', 'р','с', 'т', 'ф', 'х', 'ц', 'ч', 'ш', 'щ']\n",
    "for i in satistic.title:\n",
    "    consonants.append(sum(letter in consonant for letter in i))\n",
    "satistic['consonants'] = consonants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_st = satistic[satistic.is_train]\n",
    "test_st = satistic[~satistic.is_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = satistic.merge(data_all, how = 'left', on = 'doc_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fata_tf_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-b65286e40892>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m ngram_range=(1,1))\n\u001b[0;32m      5\u001b[0m \u001b[0mdata_tf_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdata_tf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata_tf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfata_tf_1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'fata_tf_1' is not defined"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df = 0.8, min_df = 0.2,\n",
    "dtype = np.float32,\n",
    "use_idf=True,\n",
    "ngram_range=(1,1))\n",
    "data_tf_1 = vectorizer.fit_transform(data_all.text)\n",
    "data_tf = hstack([data_tf, data_tf_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Копипаст Севы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28026\n"
     ]
    }
   ],
   "source": [
    "doc_to_title = {}\n",
    "with open('titles_norm_ru.csv', encoding='utf-8', newline='') as f:\n",
    "    for num_line, line in enumerate(f):\n",
    "        if num_line == 0:\n",
    "            continue\n",
    "        data = line.strip().split('\\t', 1)\n",
    "        \n",
    "        doc_id = int(data[0])\n",
    "        if len(data) == 1:\n",
    "            title = ''\n",
    "        else:\n",
    "            title = data[1]\n",
    "        doc_to_title[doc_id] = title\n",
    "print (len(doc_to_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('kirill/train_groups.csv')\n",
    "traingroups_titledata = {}\n",
    "for i in range(len(train_data)):\n",
    "    new_doc = train_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    target = new_doc['target']\n",
    "    title = doc_to_title[doc_id]\n",
    "    if doc_group not in traingroups_titledata:\n",
    "        traingroups_titledata[doc_group] = []\n",
    "    traingroups_titledata[doc_group].append((doc_id, title, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 22) (11690,) (11690,)\n"
     ]
    }
   ],
   "source": [
    "y_train = []\n",
    "X_train = []\n",
    "groups_train = []\n",
    "for new_group in traingroups_titledata:\n",
    "    docs = traingroups_titledata[new_group]\n",
    "    for k, (doc_id, title, target_id) in enumerate(docs):\n",
    "        y_train.append(target_id)\n",
    "        groups_train.append(new_group)\n",
    "        all_dist = []\n",
    "        words = set(title.strip().split())\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j, target_j = docs[j]\n",
    "            words_j = set(title_j.strip().split())\n",
    "            all_dist.append(len(words.intersection(words_j)))\n",
    "        X_train.append(sorted(all_dist, reverse=True)[0:19]    )\n",
    "X_train = np.column_stack((X_train, train_st[['numbers','vowels', 'consonants']]))\n",
    "# X_train = np.column_stack((X_train, new_train[['mean_true_hash']]))\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "groups_train = np.array(groups_train)\n",
    "print (X_train.shape, y_train.shape, groups_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('kirill/test_groups.csv')\n",
    "testgroups_titledata = {}\n",
    "for i in range(len(test_data)):\n",
    "    new_doc = test_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    title = doc_to_title[doc_id]\n",
    "    if doc_group not in testgroups_titledata:\n",
    "        testgroups_titledata[doc_group] = []\n",
    "    testgroups_titledata[doc_group].append((doc_id, title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627, 22) (16627,)\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "groups_test = []\n",
    "for new_group in testgroups_titledata:\n",
    "    docs = testgroups_titledata[new_group]\n",
    "    for k, (doc_id, title) in enumerate(docs):\n",
    "        groups_test.append(new_group)\n",
    "        all_dist = []\n",
    "        words = set(title.strip().split())\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j = docs[j]\n",
    "            words_j = set(title_j.strip().split())\n",
    "            all_dist.append(len(words.intersection(words_j)))\n",
    "        X_test.append(sorted(all_dist, reverse=True)[0:19]    )\n",
    "X_test = np.column_stack((X_test, test_st[['numbers','vowels', 'consonants']]))\n",
    "# X_test = np.column_stack((X_test, new_test[['mean_true_hash']]))\n",
    "X_test = np.array(X_test)\n",
    "groups_test = np.array(groups_test)\n",
    "print (X_test.shape, groups_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make TfIdf of all text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "model = Pipeline([\n",
    "    ('tf', TfidfVectorizer()),\n",
    "    ('clf', XGBClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28317, 2129)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.674916706330319\n"
     ]
    }
   ],
   "source": [
    "groups = train_data.group_id.unique()\n",
    "length = len(train_data[train_data['group_id'].isin(groups[:int(len(groups)*4/6)])])\n",
    "\n",
    "train_tf = data_tf.tocsr()[:len(train_data)]\n",
    "test_tf = data_tf.tocsr()[len(train_data):]\n",
    "\n",
    "train_new = hstack([train_tf, X_train])\n",
    "test_new = hstack([test_tf, X_test])\n",
    "    \n",
    "X = train_new.tocsc()[:length]\n",
    "X_val = train_new.tocsc()[length:]\n",
    "y = y_train[:length]\n",
    "y_val = y_train[length:]\n",
    "model = XGBClassifier(max_depth = 4, learning_rate = 0.01, \n",
    "                        random_state = 45, n_estimators  = 500, n_jobs = -1)\n",
    "    \n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "print(f1_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_new, y_train)\n",
    "pred_xgb = model.predict(test_new)\n",
    "from collections import Counter\n",
    "Counter(pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = list(np.arange(11691, 28318))\n",
    "\n",
    "df = pd.DataFrame({'pair_id': nums, 'target': pred_xgb})\n",
    "df.to_csv('xgb_text.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
