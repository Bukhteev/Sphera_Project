{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import string\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import papermill as pm\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../data/train_groups.csv')\n",
    "test  = pd.read_csv('../../data/test_groups.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "for i in range(1,28025):\n",
    "    filename = '../../data/new_content/' + str(i) + '.dat'\n",
    "    try:\n",
    "        with open(filename) as f:\n",
    "            text.append(f.read())\n",
    "    except:\n",
    "        text.append(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Текстовые признаки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text to dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_doc(doc):\n",
    "    \"\"\"\n",
    "        Convert the input document into a list of tokens, discarding all punctuation and lowercasing the tokens\n",
    "        doc: string\n",
    "\n",
    "        return list of strings\n",
    "    \"\"\"\n",
    "    # discard all punctuation\n",
    "    table = str.maketrans({key: None for key in string.punctuation})\n",
    "#     print(table)\n",
    "    doc = doc.translate(table)\n",
    "\n",
    "    # replace all whitespace characters with just space\n",
    "\n",
    "    # split doc into tokens by space\n",
    "\n",
    "    # discard empty tokens and lowercase\n",
    "    tokens = doc.lower().split() \n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(docs, min_count=None):\n",
    "    \"\"\"\n",
    "        Build the vocaublary mapping (that is, the correspondance between the token and its numeric id)\n",
    "        docs: a list of tokenized documents\n",
    "        min_count (optional): int, discard tokens that appeared less than min_count times\n",
    "\n",
    "        return dictionary str -> int\n",
    "    \"\"\"\n",
    "\n",
    "    # count all tokens in all documents and filter those that appear less than min_count\n",
    "    if min_count is not None:\n",
    "        count = dict(Counter(sum(docs,[])))\n",
    "        \n",
    "        count = Counter(dict(filter(lambda x: x[1] >= min_count, count.items())))\n",
    "        q = sorted(count)\n",
    "        \n",
    "        vocab = {q[x]: x for x in range(len(q))}\n",
    "        \n",
    "    else:\n",
    "        count = dict(Counter(sum(docs,[])))\n",
    "        q = sorted(count)\n",
    "        vocab = {q[x]: x for x in range(len(q))}\n",
    "                     \n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_multihot(doc, vocab):\n",
    "    \"\"\"\n",
    "        Convert a document to a multihot representation\n",
    "        doc: str, a tokenized document\n",
    "        vocab: dict, vocabulary mapping\n",
    "\n",
    "        return np.array, shape=(|V|,)\n",
    "    \"\"\"\n",
    "\n",
    "    # create a vector of zeros of the shape (|V|, )\n",
    "    x = np.zeros(len(vocab.keys()))\n",
    "    \n",
    "    # set the corresponding dimensions to 1\n",
    "    \n",
    "    for j in doc:\n",
    "        if j in vocab.keys():\n",
    "            x[vocab[j]] = 1\n",
    "    \n",
    "    \n",
    "\n",
    "    # set the corresponding dimensions to 1\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_onehot(labels):\n",
    "    \"\"\"\n",
    "        Convert the indices to one-hot representation\n",
    "        labels: np.array of labels, shape=(N,)\n",
    "\n",
    "        return np.array, shape=(N, k)\n",
    "       \"\"\"\n",
    "    \n",
    "    n_classes = len(set(labels))\n",
    "    n_samples = len(labels)\n",
    "\n",
    "    # create a matrix of zeros of shape (n_samples, n_classes)\n",
    "    one_hot = np.zeros((n_samples,n_classes))\n",
    "    # fill one-hot values\n",
    "    one_hot[np.arange(n_samples),labels ] = 1\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_x = [tokenize_doc(d) for d in text]\n",
    "x_vocab = build_vocab(tokenize_x, min_count = 10)\n",
    "docs_toy_multi_hot = [doc_to_multihot(i, x_vocab) for i in tokenize_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1., 0., 0., ..., 0., 1., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([1., 1., 0., ..., 1., 1., 1.]),\n",
       " array([0., 1., 0., ..., 0., 0., 1.]),\n",
       " array([1., 0., 0., ..., 0., 1., 1.]),\n",
       " array([1., 1., 0., ..., 0., 0., 0.]),\n",
       " array([1., 0., 0., ..., 0., 1., 0.]),\n",
       " array([0., 1., 0., ..., 0., 0., 1.]),\n",
       " array([1., 0., 0., ..., 0., 0., 0.]),\n",
       " array([1., 1., 0., ..., 0., 0., 1.]),\n",
       " array([1., 0., 0., ..., 0., 0., 0.]),\n",
       " array([1., 1., 0., ..., 0., 0., 0.]),\n",
       " array([1., 0., 0., ..., 0., 0., 0.]),\n",
       " array([1., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([1., 1., 1., ..., 1., 0., 0.]),\n",
       " array([1., 0., 0., ..., 0., 0., 0.]),\n",
       " array([1., 1., 0., ..., 0., 0., 1.]),\n",
       " array([1., 0., 0., ..., 0., 0., 1.])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_toy_multi_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('multi_hot.csv', docs_toy_multi_hot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
